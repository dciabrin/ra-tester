#!/bin/bash

# This file is part of ratester.
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 3
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA.

# ---- User config

# path of the cloud image to use for creating VMs that will host the cluster
IMG=

# Name prefix for the VMs to be created.
VMPREFIXNAME=ratester

# libvirt networks to use for the VM. If those networks do not exist,
# the script will create them.
# A few notes on network properties if you intent to provide yours:
#   . A first network SSHNW is both used for SSH connection and for
#     fencing requests. SSHNW provides relies on DHCP to give guests
#     their IP address, and provide NAT'd access to external world.
#     No cluster service runs on it.
#   . A second network CLUSTERNW is where all cluster services run
#     (pacemaker, corosync, resource's own communication...)
#     CLUSTERNW has DHCP disabled to avoid issue with cluster
#     manager when leases expire. IP are generated by ra-tester and
#     assigned automatically via cloud-init
CLUSTERNW=ratester
CLUSTERNWCIDR_DEFAULT=172.16.0.0/12
CLUSTERNWCIDR=
SSHNW=ratesterssh

# name of the storage pool that holds ra-tester VM's images
VMPOOLNAME=ratester

# Optional SSH public key to install on all created VM
ADDITIONALSSHKEY= #$HOME/.ssh/id_rsa.pub

# Optional packages to install on all created VM
ADDITIONALPKGS= #emacs-nox,vim

# Additional libvirt customize setup
ADDITIONALCMDS= #

# whether we enable SELinux on hosts or not
SELINUX=0
# shall we allow recreating VM if they already exist?
TEARDOWN=0
# keep tmp files for debugging problems
SAVETEMPS=0

VFLAGS=

# ---- Obscure config

VMRAM=1024
VMCPU=1
VMTMPPATH=$PWD/tmp
VMSTORAGEPATH=$PWD/vm
VMBASEIMG=$VMTMPPATH/ratmp-base.qcow2
QEMUURI=qemu:///system
CMDTIMEOUT=300
SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
LOGINKEYNAME=login-key
FENCEKEYNAME=fence-key
FENCEXVMKEYNAME=ratester_fence_xvm.key

# Generated based on user config
SSHMAC=
CLUSTERMAC=
CLUSTERNWGW=

# Distro-specific settings
PKG_FORMAT=
DISTRO=
DISTRO_PKGS=$(cat<<PKG
{
"rpm-rhel": {"mariadb-galera":"mariadb-galera-server"},
"rpm-fedora": {"mariadb-galera":"mariadb-server",
               "fence-agents-virsh":"fence-agents-virsh"},
"rpm-centos": {"mariadb-galera":"mariadb-server",
               "galera-dbg":"galera-debuginfo"},
"rpm-opensuse": {"fence-virtd":"fence-agents",
                 "fence-agents-virsh":"fence-agents",
                 "fence-agents-xvm":"fence-agents",
                 "mariadb-galera":"mariadb-server",
                 "galera":"galera-3",
                 "galera-dbg":"galera-3",
                 "garbd":"galera-3"},
"rpm": {"dig":"bind-utils",
        "fence-agents-xvm": "fence-virt",
        "galera":"galera",
        "galera-dbg":"galera",
        "garbd":"galera"},
"deb": {"dig":"dnsutils",
        "pacemaker-cts":"pacemaker",
        "fence-virtd":"fence-agents",
        "fence-agents-virsh":"fence-agents",
        "fence-agents-xvm":"fence-agents",
        "mariadb-galera":"mariadb-server",
        "galera":"galera-3",
        "galera-dbg":"galera-3",
        "garbd":"galera-arbitrator-3"
       }
}
PKG
)

usage()
{
    cat <<EOF
Create and set up KVM-based virtual machines for ra-tester
Usage: $(basename $0) --img QCOW2_IMAGE_PATH --name VM_PREFIX_NAME [options ...]

Options:
   --img QCOW2_IMAGE_PATH       QCOW2 cloud image to be used as a base for creating VMs
   --name VM_PREFIX_NAME        Name prefix for the VMs to be created (default: ratester)
   --cluster-nw NW_NAME         Libvirt network for pacemaker and resources (no DHCP)
   --ssh-nw NW_NAME             Secondary network, for debugging purpose
   --opt-ssh-key SSH_KEY        Optional public key to inject into created VMs
   --opt-pkgs PKG1,PKG2,...     Additional packages to install on created VMs
   --opt-cmd RUN-CMD            Additional shell command to run in the VMs before setup
   --teardown                   Delete VMs and associated storage if they already exist
   --selinux                    Configure VMs to enable SELinux
   --verbose                    run all virsh command in verbose mode
   --save-temps                 Keep intermediate images and files

Exemple:
   $(basename $0) --img ./CentOS-7-x86_64-GenericCloud.qcow2 --name ratester
EOF
}

distro_pkg()
{
    local pkg=$1
    echo $DISTRO_PKGS | python -c "import json,sys; j=json.load(sys.stdin); r=j.get('$PKG_FORMAT-$DISTRO',{}).get('$pkg',False); print r if r else j.get('$PKG_FORMAT',{}).get('$pkg','$pkg')"
}

cleanup_tmp_files()
{
    if [ $SAVETEMPS -ne 1 ]; then
        rm -f ${VMTMPPATH}/ratmp-* ${VMTMPPATH}/*-data* ${VMTMPPATH}/*.rules
    fi
}

on_error()
{
    echo "ERROR: failure occurred in $(basename $0):$1. Aborting" >&2
    cleanup_tmp_files
}

define_networks()
{
    trap 'on_error $LINENO' ERR

    if ! virsh -c ${QEMUURI} net-list | grep -q "${CLUSTERNW}\\s"; then
        echo "Creating cluster network \"$CLUSTERNW\" (no dhcp)"
        CLUSTERNW=$CLUSTERNW \
        CLUSTERNWGW=$CLUSTERNWGW \
        CLUSTERNWMASK=$CLUSTERNWMASK \
        envsubst <templates/cluster-net.xml.in >${VMTMPPATH}/ratmp-cluster-net.xml
        virsh -c ${QEMUURI} net-define ${VMTMPPATH}/ratmp-cluster-net.xml
        virsh -c ${QEMUURI} net-autostart ${CLUSTERNW}
        virsh -c ${QEMUURI} net-start ${CLUSTERNW}
    fi
    if ! virsh -c ${QEMUURI} net-list | grep -q "${SSHNW}\\s"; then
        echo "Creating cluster network \"$SSHNW\" (dhcp, external access)"
        SSHNW=$SSHNW envsubst <templates/ssh-net.xml.in >${VMTMPPATH}/ratmp-ssh-net.xml
        virsh -c ${QEMUURI} net-define ${VMTMPPATH}/ratmp-ssh-net.xml
        virsh -c ${QEMUURI} net-autostart ${SSHNW}
        virsh -c ${QEMUURI} net-start ${SSHNW}
    fi
}

define_pool()
{
    trap 'on_error $LINENO' ERR

    if ! virsh -c ${QEMUURI} pool-list | grep -q ratester; then
        echo "Creating storage pool for ra-tester VMs"
        VMPOOLNAME=$VMPOOLNAME \
        VMSTORAGEPATH=$VMSTORAGEPATH \
        envsubst <templates/pool.xml.in >${VMTMPPATH}/ratmp-pool.xml
        virsh -c ${QEMUURI} pool-define ${VMTMPPATH}/ratmp-pool.xml
        virsh -c ${QEMUURI} pool-autostart ratester
        virsh -c ${QEMUURI} pool-start ratester
    fi
}

generate_fence_config_and_key()
{
    trap 'on_error $LINENO' ERR

    if [ ! -f fence_virt.conf ]; then
        echo "Generating configuration for multicast-based fencing"
        local iface=$(virsh -c ${QEMUURI} net-dumpxml $SSHNW | sed -ne "s/.*bridge.*name='\([^']*\)' .*/\1/p")
        QEMUURI=$QEMUURI \
        IFACE=$iface \
        FENCEXVMKEYNAME=$FENCEXVMKEYNAME \
        envsubst <templates/fence_virt.conf.in >fence_virt.conf
    fi

    if [ ! -f $FENCEXVMKEYNAME ]; then
        echo "Generating authorization key for multicast-based fencing"
        dd if=/dev/urandom of=$FENCEXVMKEYNAME bs=4096 count=1
    fi
}

ip_from_cidr()
{
    local cidr=$1
    local offset=$2
    python -c "from netaddr import *; print IPNetwork('$cidr')[$offset]"
}

mask_from_cidr()
{
    local cidr=$1
    python -c "from netaddr import *; print IPNetwork('$cidr').netmask"
}

cluster_gw()
{
    local cidr=$1
    ip_from_cidr $cidr 1
}

mac_template()
{
    local oui=$(echo $1 | md5sum | head -c +4)
    local nic=$(echo $2 | md5sum | head -c +4)
    local sub=$3
    echo ee$oui$nic$sub | sed -e 's/\(..\)/\1:/g' -e 's/:$//'
}

node_mac()
{
    local mac=$1
    local nodenum=$2
    echo "$(echo $mac | head -c +16)$nodenum"
}

node_dhcp_ip()
{
    trap 'on_error $LINENO' ERR

    local nodenum=$1
    echo $(virsh -c ${QEMUURI} net-dhcp-leases $SSHNW | awk "/"$(node_mac $SSHMAC $nodenum)"/ {print \$5}" | head -1 | sed 's%/.*%%')
}

node_static_ip()
{
    trap 'on_error $LINENO' ERR

    local gw=$(virsh -c ${QEMUURI} net-dumpxml $CLUSTERNW | sed -ne "s/.*ip.*address='\([^']*\)'.*/\1/p")
    local mask=$(virsh -c ${QEMUURI} net-dumpxml $CLUSTERNW | sed -ne "s/.*netmask='\([^']*\)'.*/\1/p")
    local hashdata=$1
    local nodenum=$2
    python -c "import hashlib, struct; from netaddr import *; ipn=IPNetwork('$gw/$mask'); h=struct.unpack('I',hashlib.md5('$hashdata').digest()[:4])[0]; subip=h<<3&((1<<(32-ipn.prefixlen))-1); print ipn[subip+$nodenum]"
}

prepare_base_image()
{
    trap 'on_error $LINENO' ERR

    local etc_hosts=$(for i in 1 2 3; do echo -n "$(node_static_ip $VMPREFIXNAME $i) $VMPREFIXNAME$i.ratester $VMPREFIXNAME$i\n"; done)

    echo "Creating temporary base image based on ${IMG}"
    cp ${IMG} ${VMBASEIMG}

    echo "Determining package format in base image ${IMG}"
    local inspect=$(virt-inspector -a ${VMBASEIMG})
    DISTRO=$(echo $inspect | virt-inspector --xpath '//operatingsystems/operatingsystem/distro/text()')
    PKG_FORMAT=$(echo $inspect | virt-inspector --xpath '//operatingsystems/operatingsystem/package_format/text()')

    echo "Preparing temporary base image"
    # startup and access
    virt-customize ${VFLAGS} -a ${VMBASEIMG} \
        --run-command "\`which echo\` -e \"$etc_hosts\" >> /etc/hosts" \
        --timezone $(ls -l /etc/localtime | sed -s 's%.*/usr/share/zoneinfo/%%') \
        --run-command "echo 'PermitRootLogin yes' >> /etc/ssh/sshd_config" \
        --mkdir "/root/.ssh" \
        --upload $LOGINKEYNAME:/root/.ssh/id_rsa \
        --upload $LOGINKEYNAME.pub:/root/.ssh/id_rsa.pub \
        --run-command "cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys" \
        --upload $FENCEKEYNAME:/root/.ssh/$FENCEKEYNAME \
        --upload $FENCEKEYNAME.pub:/root/.ssh/$FENCEKEYNAME.pub

    # optional pre-configure commands to be run, typically set by user to add repos
    if [ "$ADDITIONALCMDS" != "" ]; then
        echo -ne "$ADDITIONALCMDS" | xargs -0 virt-customize ${VFLAGS} -a ${VMBASEIMG}
    fi

    # ensure lp#1577982 is fixed otherwise cloud-init fail to assign ip properly
    virt-customize ${VFLAGS} -a ${VMBASEIMG} \
        --run-command "sed -i \"s/'network-config': {}/'network-config': None/\" /usr/lib/python*/site-packages/cloudinit/sources/DataSourceNoCloud.py"

    # pacemaker + fencing
    virt-customize ${VFLAGS} -a ${VMBASEIMG} \
        --install pacemaker,$(distro_pkg pacemaker-cts) \
        --install pcs,resource-agents \
        --install $(distro_pkg fence-virtd),$(distro_pkg fence-agents-virsh),$(distro_pkg fence-agents-xvm) \
        --password hacluster:password:ratester \
        --run-command "systemctl enable pcsd" \
        --mkdir /etc/cluster \
        --upload $FENCEXVMKEYNAME:/etc/cluster/fence_xvm.key

    if [ "$PKG_FORMAT" == "deb" ]; then
        # fix pcsd dependency and install manually
        # fix pacemaker and pacemaker-remote .deb mutually exclusion (why?)
        # fix pacemaker default runlevels for systemctl enable/disable
        virt-customize ${VFLAGS} -a ${VMBASEIMG} \
             --run-command "gem install orderedhash" \
             --run-command "sed -i \"s/'::',/'0.0.0.0',/\" /usr/share/pcsd/ssl.rb" \
             --run-command "sed -i 's%find /var/lib%find /var/lib/pacemaker/cib%' /usr/lib/python2.7/dist-packages/pcs/cluster.py" \
             --run-command "rm -f /etc/corosync/corosync.conf" \
             --mkdir /var/log/cluster \
             --run-command "touch /var/log/cluster/corosync.log" \
             --run-command "apt-get download pacemaker-remote" \
             --run-command "dpkg -x pacemaker-remote*.deb /" \
             --run-command "for service in pacemaker pacemaker_remote; do sed -i -e 's/\(Default-Start:\)$/\1\t2 3 4 5/' -e 's/\(Default-Stop:\)$/\1 \t0 1 6/' /etc/init.d/\$service; done" \
             --run-command "for service in pacemaker pacemaker_remote; do for i in 0 1 2 3 4 5 6; do cd /etc/rc\$i.d && ln -s ../init.d/\$service K01\$service; done; done"
    elif [ "$PKG_FORMAT" == "rpm" -a "$DISTRO" == "opensuse" ]; then
        virt-customize ${VFLAGS} -a ${VMBASEIMG} \
             --run-command "sed -i \"s/'::'/'0.0.0.0'/\" /usr/lib/pcsd/ssl.rb" \
             --run-command "sed -i 's/\(\S\S*\)\(\s\s*\)\(\S\S*\)\(\s\s*\)\(\S\S*\)/\1\2\3\4common-\1/' /etc/pam.d/pcsd" \
             --run-command "sed -i 's%/usr/libexec/%/usr/lib/%' /usr/lib/pcsd/settings.rb" \
             --run-command "sed -i 's%/usr/libexec/%/usr/lib/%' /usr/lib/python2.7/site-packages/pcs/settings_default.py" \
             --install pacemaker-remote
             # --run-command "sed -i 's/self._apply_hostname(hostname)/self._apply_hostname(writeable_hostname)/' /usr/lib/python2.7/site-packages/cloudinit/distros/__init__.py"
             # --run-command "grub2-mkconfig > /boot/grub2/grub.cfg" # <- to avoid reboot issues
    else
        virt-customize ${VFLAGS} -a ${VMBASEIMG} --install pacemaker-remote
    fi

    # galera: mysql-server/galera, galera-replication/debugsymbols, garbd
    virt-customize ${VFLAGS} -a ${VMBASEIMG} \
                   --install $(distro_pkg docker) \
                   --install $(distro_pkg mariadb-galera) \
                   --install $(distro_pkg galera) \
                   --install $(distro_pkg galera-dbg) \
                   --install $(distro_pkg garbd) \
                   --run-command "if ! systemctl disable mysql 2>/dev/null; then systemctl disable mariadb; fi"

    if [ "$PKG_FORMAT" == "deb" ]; then
        # TODO: check why resource agent spawns galera with socket
        # settings which are different from default :/
        virt-customize ${VFLAGS} -a ${VMBASEIMG} \
            --run-command "sed -i -e 's%socket\s*=.*%socket\t\t= /var/lib/mysqld/mysqld.sock%' /etc/mysql/my.cnf"
    fi

    # dependencies for RA
    virt-customize ${VFLAGS} -a ${VMBASEIMG} --install screen,gdb,$(distro_pkg dig)

    # SELinux settings
    if [ $SELINUX -eq 1 ]; then
        if [ "$PKG_FORMAT" == "rpm" -a "$DISTRO" == "rhel" ]; then
            virt-customize ${VFLAGS} -m 1024 -a ${VMBASEIMG} --install openstack-selinux
        fi
    else
        virt-customize ${VFLAGS} -a ${VMBASEIMG} \
           --run-command "if [ -f /etc/selinux/config ]; then sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config; fi"
    fi

    # convenience packages
    if [ ! -z "$ADDITIONALPKGS" ]; then
        virt-customize ${VFLAGS} -a ${VMBASEIMG} --install $ADDITIONALPKGS
    fi

    # convenience ssh key
    if [ -f "$ADDITIONALSSHKEY" ]; then
        virt-customize ${VFLAGS} -a ${VMBASEIMG} --ssh-inject root:file:$ADDITIONALSSHKEY
    fi
}

setup_node()
{
    trap 'on_error $LINENO' ERR

    local nodenum=$1

    local IMGNAME=${VMPREFIXNAME}${nodenum}.qcow2
    local ISONAME=${VMPREFIXNAME}${nodenum}.iso

    if virsh -c ${QEMUURI} dominfo ${VMPREFIXNAME}${nodenum} &>/dev/null; then
        if virsh -c ${QEMUURI} domstate ${VMPREFIXNAME}${nodenum} | grep -q running; then
            echo "Stopping previously defined VM ${VMPREFIXNAME}${nodenum}..."
            virsh -c ${QEMUURI} destroy ${VMPREFIXNAME}${nodenum}
        fi
        echo "Deleting previously defined VM ${VMPREFIXNAME}${nodenum}..."
        virsh -c ${QEMUURI} undefine ${VMPREFIXNAME}${nodenum} --remove-all-storage
    fi
    if virsh -c ${QEMUURI} vol-info --pool ${VMPOOLNAME} ${IMGNAME} &>/dev/null; then
       virsh -c ${QEMUURI} vol-delete --pool ${VMPOOLNAME} ${IMGNAME} 2>/dev/null
    fi
    if virsh -c ${QEMUURI} vol-info --pool ${VMPOOLNAME} ${ISONAME} &>/dev/null; then
       virsh -c ${QEMUURI} vol-delete --pool ${VMPOOLNAME} ${ISONAME} 2>/dev/null
    fi

    echo "Creating storage volume info for ${VMPREFIXNAME}${nodenum}"
    mkdir -p $VMSTORAGEPATH
    virsh -c ${QEMUURI} vol-create-as ${VMPOOLNAME} ${IMGNAME} 10GB --format qcow2
    virsh -c ${QEMUURI} vol-create-as ${VMPOOLNAME} ${ISONAME} 500KB --format raw

    echo "Forcing order of network interface cards for ${VMPREFIXNAME}${nodenum}"
    SSHMAC=$(node_mac $SSHMAC $nodenum) \
    CLUSTERMAC=$(node_mac $CLUSTERMAC $nodenum) \
    envsubst <templates/udev-90-ratester-nic-order.rules.in >${VMTMPPATH}/90-ratester-nic-order.rules

    echo "Creating image for ${VMPREFIXNAME}${nodenum}"
    # we could let cloudinit set hostname, but doing it here
    # has the proper hostname propagated to dnsmasq's DHCP on the host
    # NOTE: do a special dance to prevent --hostname to update /etc/hosts
    virt-customize ${VFLAGS} -a ${VMTMPPATH}/ratmp-base.qcow2 \
        --upload ${VMTMPPATH}/90-ratester-nic-order.rules:/etc/udev/rules.d \
        --move /etc/hosts:/etc/hosts.tmp \
        --hostname ${VMPREFIXNAME}${nodenum} \
        --move /etc/hosts.tmp:/etc/hosts
    virsh -c ${QEMUURI} vol-upload --pool ${VMPOOLNAME} ${IMGNAME} ${VMTMPPATH}/ratmp-base.qcow2

    echo "Creating cloud-init metadata for ${VMPREFIXNAME}${nodenum}"
    DOMNAME=${VMPREFIXNAME}${nodenum} \
    CLUSTERIP=$(node_static_ip $VMPREFIXNAME $nodenum) \
    CLUSTERNWGW=$CLUSTERNWGW \
    CLUSTERNWMASK=$CLUSTERNWMASK \
    envsubst <templates/cloudinit-meta-data.in >${VMTMPPATH}/meta-data
    DOMNAME=${VMPREFIXNAME}${nodenum} \
    envsubst <templates/cloudinit-user-data.in >${VMTMPPATH}/user-data
    (cd ${VMTMPPATH} && genisoimage -output ${VMTMPPATH}/ratmp-${ISONAME} -volid cidata -joliet -rock user-data meta-data)
    virsh -c ${QEMUURI} vol-upload --pool ${VMPOOLNAME} ${ISONAME} ${VMTMPPATH}/ratmp-${ISONAME}

    echo "Creating ${VMPREFIXNAME}${nodenum} and waiting for its IP to be assigned by DHCP"
    DOMNAME=${VMPREFIXNAME}${nodenum} \
    VMRAM=${VMRAM} \
    VMCPU=${VMCPU} \
    IMGNAME=${IMGNAME} \
    ISONAME=$ISONAME \
    VMPOOLNAME=$VMPOOLNAME \
    SSHNW=$SSHNW \
    SSHMAC=$(node_mac $SSHMAC $nodenum) \
    CLUSTERNW=$CLUSTERNW \
    CLUSTERMAC=$(node_mac $CLUSTERMAC $nodenum) \
    envsubst <templates/domain.xml.in >${VMTMPPATH}/ratmp-${VMPREFIXNAME}${nodenum}.xml

    virsh -c ${QEMUURI} define ${VMTMPPATH}/ratmp-${VMPREFIXNAME}${nodenum}.xml
    virsh -c ${QEMUURI} start ${VMPREFIXNAME}${nodenum}
    timeout $CMDTIMEOUT sh -c "while ! (virsh -c ${QEMUURI} net-dhcp-leases $SSHNW | grep -q $(node_mac $SSHMAC $nodenum)); do sleep 2; done"
}

wait_for_node()
{
    trap 'on_error $LINENO' ERR

    local nodenum=$1
    local nodeip=$(node_dhcp_ip $nodenum)

    # the node may reboot when first started, so loop here
    # until ssh is successful, rather than just ping
    echo "Wait until we can connect to ${VMPREFIXNAME}${nodenum}"
    timeout $CMDTIMEOUT sh -c "while ! ssh -q -i $LOGINKEYNAME $SSH_OPTS root@$nodeip true 2>/dev/null; do sleep 2; done"
    if [ $? -ne 0 ]; then
        echo "ERROR: Failed to get SSH connection to domain ${VMPREFIXNAME}${nodenum}" >&2
        exit 1
    fi

    # tmp for opensuse: fix reboot
    if [ "$PKG_FORMAT" == "rpm" -a "$DISTRO" == "opensuse" ]; then
        echo "Update grub to fix reboot on ${VMPREFIXNAME}${nodenum}"
        ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "grub2-mkconfig > /boot/grub2/grub.cfg"
    fi

    # cloud-init and nocloud ds: ensure that cluster nw nic is brought up
    echo "Ensure reboot succeeds and cluster network is up"
    ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "reboot && cat" || true
    local clustermac=$(node_mac $CLUSTERMAC $nodenum)
    timeout $CMDTIMEOUT sh -c "while ! ssh -q -i $LOGINKEYNAME $SSH_OPTS root@$nodeip \"ip link | grep -B1 $clustermac | head -1 | grep -q 'state UP'\" 2>/dev/null; do sleep 2; done"
    if [ $? -ne 0 ]; then
        echo "ERROR: Failed to get verify cluster network status on domain ${VMPREFIXNAME}${nodenum}" >&2
        exit 1
    fi
}

wait_for_pcsd()
{
    trap 'on_error $LINENO' ERR

    local nodenum=$1
    local nodeip=$(node_dhcp_ip $nodenum)

    # SSH is not enough, pcsd has to be up and running to be able
    # to create a cluster from a single node
    echo "Wait until pcsd is up and running ${VMPREFIXNAME}${nodenum}"
    timeout $CMDTIMEOUT sh -c "while ! ssh -q -i $LOGINKEYNAME $SSH_OPTS root@$nodeip \"netstat -tnlp | grep -q ':2224\\s'\" 2>/dev/null; do sleep 2; done"
    if [ $? -ne 0 ]; then
        echo "ERROR: Failed to get pcsd status on domain ${VMPREFIXNAME}${nodenum}" >&2
        exit 1
    fi
}

ensure_teardown_if_vm_exist()
{
    for nodenum in 1 2 3; do
        if virsh -c ${QEMUURI} dominfo ${VMPREFIXNAME}${nodenum} &>/dev/null; then
            if [ $TEARDOWN -eq 0 ]; then
                echo "ERROR: VM ${VMPREFIXNAME}${nodenum} already exists. Use option --teardown to allow ra-tester to recreate VMs" >&2
                exit 1
            else
                return 0
            fi
        fi
    done
}

generate_ssh_keys()
{
    trap 'on_error $LINENO' ERR

    if [ ! -f $LOGINKEYNAME ]; then
        echo "Creating SSH keypair for logging in"
        ssh-keygen -t rsa -f $LOGINKEYNAME -N "" -C 'ratester login key'
    fi
    if [ ! -f $FENCEKEYNAME ]; then
        echo "Creating SSH keypair for node fencing"
        ssh-keygen -t rsa -f $FENCEKEYNAME -N "" -C 'ratester fence key'
        cat $FENCEKEYNAME.pub >> $HOME/.ssh/authorized_keys
    fi
}

init_cluster()
{
    trap 'on_error $LINENO' ERR

    local nodeip=$(node_dhcp_ip 1)

    echo "Initializing pacemaker/corosync config"
    local cluster_nodes=$(for i in 1 2 3; do echo ${VMPREFIXNAME}$i; done | xargs echo)
    ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "pcs cluster auth ${cluster_nodes} -u hacluster -p ratester"
    ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "pcs cluster setup --force --name ratester ${cluster_nodes}"
    ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "pcs cluster start --all"
    # give cluster time to settle, and then disable stonith initially
    ssh -i $LOGINKEYNAME $SSH_OPTS root@$nodeip "timeout 60 /bin/sh -c 'while ! pcs property set stonith-enabled=false; do sleep 2; done'"
}

generate_ssh_config()
{
    trap 'on_error $LINENO' ERR

    echo "Generating SSH config for logging in"

    VMPREFIXNAME=${VMPREFIXNAME} \
    LOGINKEYNAME=${PWD}/${LOGINKEYNAME} \
    envsubst <templates/ssh_config.in >ssh_config_${VMPREFIXNAME}
    for i in 1 2 3; do
        local node=${VMPREFIXNAME}$i
        local nodeip=$(node_dhcp_ip $i)
        echo -e "\nHost ${node}\n\tHostName ${nodeip}" >>ssh_config_${VMPREFIXNAME}
    done
}


while [ "$1" != "" ]; do
    case "$1" in
        --img ) IMG="$2"; shift 2;;
        --name ) VMPREFIXNAME="$2"; shift 2;;
        --cluster-nw ) CLUSTERNW="$2"; SSHNW="$2"; shift 2;;
        --cluster-nw-cidr ) CLUSTERNWCIDR="$2"; shift 2;;
        --ssh-nw ) SSHNW="$2"; shift 2;;
        --opt-ssh-key ) ADDITIONALSSHKEY="$2"; shift 2;;
        --opt-pkgs ) ADDITIONALPKGS="$2"; shift 2;;
        --opt-cmd )
            if [ "$ADDITIONALCMDS" != "" ]; then
                ADDITIONALCMDS="$ADDITIONALCMDS\0000"
            fi
            ADDITIONALCMDS="$ADDITIONALCMDS--run-command\0000$2"; shift 2;;
        --teardown ) TEARDOWN="1"; shift;;
        --selinux ) SELINUX=1; shift;;
        --verbose ) VFLAGS="$VFLAGS -v"; shift;;
        --save-temps ) SAVETEMPS=1; shift;;
        -h|--help ) usage; exit 0;;
        * ) echo "ERROR: unknown parameter \"$1\"" >&2
            usage
            exit 1;;
    esac
done

if [ "$IMG" == "" ]; then
    echo "ERROR: no qcow2 image provided to build VM. Use --img QCOW2_IMAGE_PATH" >&2
    exit 1
fi

if ! virsh -c ${QEMUURI} list &>/dev/null; then
    echo "ERROR: Cannot connect to hypervisor at ${QEMUURI}" >&2
    echo "Check that user belongs to group 'kvm' or has proper sudo settings" >&2
    exit 1
fi

set -e

# Creation and provisioning
# -------------------------
test -d $VMTMPPATH || mkdir -p $VMTMPPATH
test -d $VMSTORAGEPATH || mkdir -p $VMSTORAGEPATH



SSHMAC=$(mac_template ratester $VMPREFIXNAME 00)
CLUSTERMAC=$(mac_template ratester $VMPREFIXNAME f0)
CLUSTERNWCIDR=${CLUSTERNWCIDR:-$CLUSTERNWCIDR_DEFAULT}
CLUSTERNWGW=$(cluster_gw $CLUSTERNWCIDR)
CLUSTERNWMASK=$(mask_from_cidr $CLUSTERNWCIDR)

# Pre-check: ensure ra-tester can delete VMs if they already exist
ensure_teardown_if_vm_exist

# Generate ratester keys, keep if already present
generate_ssh_keys

# # Create networks in libvirt, reuse if existing
define_networks

# Create storage pool in libvirt, reuse if existing
define_pool

# Generate fencing config for fence_xvm
generate_fence_config_and_key

# Create the and customize the base image
prepare_base_image

# Create VMs from the base image and finish provisioning
for i in 1 2 3; do setup_node $i; done

# # Dump connection setting
generate_ssh_config

# Wait for VMs availability
for i in 1 2 3; do wait_for_node $i; done

# Wait for pcsd availability
for i in 1 2 3; do wait_for_pcsd $i; done

# Setup a pacemaker cluster on the created VM
init_cluster

cleanup_tmp_files

cat <<EOF
Cluster bootstrapped. You may now log in with:
   ssh -F ssh_config_${VMPREFIXNAME} ${VMPREFIXNAME}{x}

You're almost ready to run ra-tester!
------------------------------------------------------
* Finish fencing configuration manually
If you intend to use SSH-based fencing (fence_virsh):
   . Please add SSH public key ${FENCEKEYNAME}.pub to your .ssh/authorized_keys
   . you'll need to run ra-tester with "--stonith 1"

If you intend to use Multicast-based fencing (fence_xvm):
   . Please copy the generated fence_virt.conf to /etc/fence_virt.conf
   . Please copy the generated xvm fence key ${FENCEXVMKEYNAME} to /etc/cluster/
   . you'll need to run ra-tester with "--stonith xvm"
EOF
